AWSTemplateFormatVersion: '2010-09-09'
Description: Retrieves RDS Metric data
Parameters:
  DatabaseName:
    Type: String
    Description: Name of the Athena database to be created to hold lambda information
    Default: optimization_data
  DataBucketsKmsKeysArns:
    Type: String
    Description: KMS Key ARNs used for encrypting data in S3 buckets (comma separated)
  DestinationBucket:
    Type: String
    Description: Name of the S3 Bucket to be created to hold data information
    AllowedPattern: (?=^.{3,63}$)(?!^(\d+\.)+\d+$)(^(([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])\.)*([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])$)
  DestinationBucketARN:
    Type: String
    Description: ARN of the S3 Bucket that exists or needs to be created to hold rightsizing information
  MultiAccountRoleName:
    Type: String
    Description: Name of the IAM role deployed in all accounts which can retrieve AWS Data.
  CFDataName:
    Type: String
    Description: The name of what this cf is doing.
    Default: rds-usage
  GlueRoleARN:
    Type: String
    Description: Arn for the Glue Crawler role
  Schedule:
    Type: String
    Description: EventBridge Schedule to trigger the data collection
    Default: "rate(14 days)"
  ResourcePrefix:
    Type: String
    Description: This prefix will be placed in front of all roles created. Note you may wish to add a dash at the end to make more readable
  RegionsInScope:
    Type: String
    Description: "Comma Delimited list of AWS regions from which data about resources will be collected. Example: us-east-1,eu-west-1,ap-northeast-1"
  LambdaAnalyticsARN:
    Type: String
    Description: Arn of lambda for Analytics
  AccountCollectorLambdaARN:
    Type: String
    Description: Arn of the Account Collector Lambda
  CodeBucket:
    Type: String
    Description: Source code bucket
  StepFunctionTemplate:
    Type: String
    Description: S3 key to the JSON template for the StepFunction
  StepFunctionExecutionRoleARN:
    Type: String
    Description: Common role for Step Function execution
  SchedulerExecutionRoleARN:
    Type: String
    Description: Common role for module Scheduler execution
  DAYS:
    Type: Number
    Description: Number of days going back that you want to get data for
    Default: 1

Outputs:
  StepFunctionARN:
    Description: ARN for the module's Step Function
    Value: !GetAtt ModuleStepFunction.Arn

Conditions:
  NeedDataBucketsKms: !Not [!Equals [!Ref DataBucketsKmsKeysArns, '']]

Resources:
  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "${ResourcePrefix}${CFDataName}-LambdaRole"
      AssumeRolePolicyDocument:
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - !Sub "lambda.${AWS::URLSuffix}"
        Version: 2012-10-17
      ManagedPolicyArns:
        - !Sub "arn:${AWS::Partition}:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
      Path: /
      Policies:
        - !If
          - NeedDataBucketsKms
          - PolicyName: "KMS"
            PolicyDocument:
              Version: "2012-10-17"
              Statement:
                - Effect: "Allow"
                  Action:
                    - "kms:GenerateDataKey"
                  Resource: !Split [ ',', !Ref DataBucketsKmsKeysArns ]
          - !Ref AWS::NoValue
        - PolicyName: "AssumeMultiAccountRole"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action: "sts:AssumeRole"
                Resource: !Sub "arn:${AWS::Partition}:iam::*:role/${MultiAccountRoleName}"
        - PolicyName: "S3Access"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action:
                  - "s3:PutObject"
                Resource:
                  - !Sub "${DestinationBucketARN}/*"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28 # Resource found with an explicit name, this disallows updates that require replacement of this resource
            reason: "Need explicit name to identify role actions"

  LambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ResourcePrefix}${CFDataName}-Lambda'
      Description: !Sub "Lambda function to retrieve ${CFDataName}"
      Runtime: python3.12
      Architectures: [x86_64]
      Code:
        ZipFile: |
          import os
          import json
          import logging
          from re import sub
          from datetime import datetime, timedelta, date

          import boto3
          from botocore.exceptions import ClientError
          from boto3.s3.transfer import S3Transfer

          #Environment Variables
          BUCKET = os.environ["BUCKET_NAME"]
          PREFIX = os.environ["PREFIX"]
          ROLE_NAME = os.environ['ROLENAME']
          REGIONS = [r.strip() for r in os.environ["REGIONS"].split(',') if r]

          logger = logging.getLogger(__name__)
          logger.setLevel(getattr(logging, os.environ.get('LOG_LEVEL', 'INFO').upper(), logging.INFO))

          functions = {
              'rds': {
                  'regional' : 1,
                  'api' : 'rds',
                  'functions' : [
                      {
                          'name' : 'get_rds_stats',
                          'output_path' : 'rds-usage-data',
                          'output_file_name' : 'rds_stats.json'
                      }
                  ]
              }
          }

          METRICS_FOR_VOLUMES = [
              'FreeableMemory',
              'CPUUtilization',
              'NetworkReceiveThroughput',
              'NetworkTransmitThroughput',
              'ReadIOPS',
              'WriteIOPS',
              'FreeStorageSpace'
          ]

          TAGS_TO_RETRIEVE = [
              'Environment',
              'Schedule',
              'Application',
              'Project',
          ]

          now = datetime.utcnow()
          DAYS = int(os.environ['DAYS'])
          past = now - timedelta(days=DAYS)
          today = datetime.today().strftime('%Y-%m-%d')
          tod = date.today()
          year = tod.year
          month = tod.strftime('%m')
          future = now# + timedelta(minutes=10)
          period = 3600

          def format_rds(rds):
              if len(rds["Attachments"]) == 0:
                  instance_id = "unattached"
                  device = ""
              else:
                  instance_id = rds["Attachments"][0]["InstanceId"] if rds["Attachments"][0]["InstanceId"] else ""
                  device = rds["Attachments"][0]["Device"] if rds["Attachments"][0]["Device"] else ""

              output = {
                  "az": rds["AvailabilityZone"],
                  "instanceId": instance_id,
                  "rdsId": rds["VolumeId"],
                  "device": device,
                  "rdsType": rds["VolumeType"],
                  "sizeGB": rds["Size"],
                  "IOPS": rds["Iops"],
                  "rdsCreateTime": rds["CreateTime"]
              }

              tags = {}
              filtered_tags = {}
              for tag in rds.get("Tags", []):
                  tags[sub('[^a-zA-Z0-9-_ *.]', '', tag["Key"].replace(",", " "))] = sub('[^a-zA-Z0-9-_ *.]', '', tag["Value"].replace(",", " "))
              for tags_required in TAGS_TO_RETRIEVE:
                  filtered_tags[tags_required] = tags[tags_required] if tags_required in tags else ''
              output["tags"] = filtered_tags
              return output

          def get_rds_ids(rds_client):
              rds_inventory = []
              rds_information = rds_client.describe_rds(Filters=[{'Name': 'status','Values':['in-use','available']}])
              if not rds_information or not rds_information['Volumes']:
                  rds_information = []
              else:
                  rds_information = rds_information['Volumes']
              for rds in rds_information:
                  rds_inventory.append(format_rds(rds))
              return rds_inventory

          def store_data_to_s3(data, region, service, path, resource_value, filename, accountID, payer_id):
              local_file = f"/tmp/{region}-{filename}"
              with open(local_file, 'w') as f:
                  json.dump(data, f, default=str)
                  f.write('\n')
              if os.path.getsize(local_file) == 0:
                  logger.info(f"No data in file for {path}")
                  return
              key = datetime.now().strftime(f"{PREFIX}/{PREFIX}-data/payer_id={payer_id}/accountid={accountID}/region={region}/year=%Y/month=%m/day=%d/{resource_value}.json")
              s3client = boto3.client('s3')
              logger.info("Uploading file %s to %s/%s" %(local_file, BUCKET, key))
              S3Transfer(s3client).upload_file(local_file, BUCKET, key, extra_args={'ACL': 'bucket-owner-full-control'})
              logger.info('file upload successful')

          def get_rds_stats(cwclient, client, s3client, region, service, path, filename, accountID, payer_id):
              datapoints = {}
              data = {}
              rds_inventory = client.describe_db_instances()
              for rds in rds_inventory['DBInstances']:
                  for metric in METRICS_FOR_VOLUMES:
                      results = cwclient.get_metric_statistics(StartTime=past, EndTime=future, MetricName=metric,
                      Namespace='AWS/RDS',Statistics=['Average','Maximum','Minimum'],Period=period,
                      Dimensions=[{"Name": "DBInstanceIdentifier", "Value": rds["DBInstanceIdentifier"]}])
                      datapoints.update({metric:results['Datapoints']})
                  rds["Datapoints"] = datapoints
                  store_data_to_s3(rds, region, service, path, rds['DBInstanceIdentifier'], filename, accountID, payer_id)

          def assume_role(account_id, service, region):
              partition = boto3.session.Session().get_partition_for_region(region_name=region)
              role_arn = f"arn:{partition}:iam::{account_id}:role/{ROLE_NAME}"
              cred = boto3.client('sts', region_name = region).assume_role(
                  RoleArn=role_arn,
                  RoleSessionName="data_collection"
              )['Credentials']
              return boto3.client(
                  service,
                  aws_access_key_id=cred['AccessKeyId'],
                  aws_secret_access_key=cred['SecretAccessKey'],
                  aws_session_token=cred['SessionToken'],
                  region_name = region
              )

          def lambda_handler(event, context):
              logger.info(f"Event: {event}")
              collection_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
              if 'account' not in event:
                  raise ValueError(
                      "Please do not trigger this Lambda manually. "
                      "Find the corresponding state machine in Step Functions and Trigger from there."
                  )

              try:
                  account = json.loads(event["account"])
                  account_id = account["account_id"]
                  account_name = account["account_name"]
                  payer_id = account["payer_id"]
                  logger.info(f"Collecting data for account: {account_id}")
                  s3client = boto3.client('s3')
                  for service in functions.keys():
                      if functions[service]['regional']:
                          for region in REGIONS:
                              logger.info(f"region {region}")
                              client = assume_role(account_id, functions[service]['api'], region)
                              for f in functions[service]['functions']:
                                  cw_client = assume_role(account_id, 'cloudwatch', region)
                                  try:
                                      data = globals()[f['name']](cw_client, client, s3client, region, service, f['output_path'], f['output_file_name'], account_id, payer_id)
                                  except Exception as e:
                                      # Send some context about this error to Lambda Logs
                                      logger.warning(e)
                      else:
                          client=boto3.client(service)
                          for f in functions[service]['functions']:
                              cw_client = boto3.client('cloudwatch', region_name = 'us-east-1')
                              try:
                                  data = globals()[f['name']](cw_client, client, s3client, 'us-east-1', service, f['output_path'], f['output_file_name'], account_id)
                              except Exception as e:
                                  # Send some context about this error to Lambda Logs
                                  logger.warning(e)
                  return "Successful"
              except Exception as e:
                  # Send some context about this error to Lambda Logs
                  logger.warning(e)


      Handler: 'index.lambda_handler'
      MemorySize: 2688
      Timeout: 300
      Role: !GetAtt LambdaRole.Arn
      Environment:
        Variables:
          BUCKET_NAME: !Ref DestinationBucket
          PREFIX: !Ref CFDataName
          ROLENAME: !Ref MultiAccountRoleName
          DAYS: !Ref DAYS
          REGIONS: !Ref RegionsInScope
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W89 # Lambda functions should be deployed inside a VPC
            reason: "No need for VPC in this case"
          - id: W92 #  Lambda functions should define ReservedConcurrentExecutions to reserve simultaneous executions
            reason: "No need for simultaneous execution"

  LogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub "/aws/lambda/${LambdaFunction}"
      RetentionInDays: 60

  Crawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Sub '${ResourcePrefix}${CFDataName}-Crawler'
      Role: !Ref GlueRoleARN
      DatabaseName: !Ref DatabaseName
      Targets:
        S3Targets:
          - Path: !Sub "s3://${DestinationBucket}/${CFDataName}/rds-usage-data/"
      Configuration: "{\"Version\":1.0,\"CrawlerOutput\":{\"Partitions\":{\"AddOrUpdateBehavior\":\"InheritFromTable\"}}}"

  ModuleStepFunction:
    Type: AWS::StepFunctions::StateMachine
    Properties:
      StateMachineName: !Sub '${ResourcePrefix}${CFDataName}-StateMachine'
      StateMachineType: STANDARD
      RoleArn: !Ref StepFunctionExecutionRoleARN
      DefinitionS3Location:
        Bucket: !Ref CodeBucket
        Key: !Ref StepFunctionTemplate
      DefinitionSubstitutions:
        AccountCollectorLambdaARN: !Ref AccountCollectorLambdaARN
        ModuleLambdaARN: !GetAtt LambdaFunction.Arn
        Crawlers: !Sub '["${ResourcePrefix}${CFDataName}-Crawler"]'
        CollectionType: "LINKED"
        Params: ''
        Module: !Ref CFDataName
        DeployRegion: !Ref AWS::Region
        Account: !Ref AWS::AccountId
        Prefix: !Ref ResourcePrefix
        Bucket: !Ref DestinationBucket

  ModuleRefreshSchedule:
    Type: 'AWS::Scheduler::Schedule'
    Properties:
      Description: !Sub 'Scheduler for the ODC ${CFDataName} module'
      Name: !Sub '${ResourcePrefix}${CFDataName}-RefreshSchedule'
      ScheduleExpression: !Ref Schedule
      State: ENABLED
      FlexibleTimeWindow:
        MaximumWindowInMinutes: 30
        Mode: 'FLEXIBLE'
      Target:
        Arn: !GetAtt ModuleStepFunction.Arn
        RoleArn: !Ref SchedulerExecutionRoleARN

  AnalyticsExecutor:
    Type: Custom::LambdaAnalyticsExecutor
    Properties:
      ServiceToken: !Ref LambdaAnalyticsARN
      Name: !Ref CFDataName

  AthenaQuery:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref DatabaseName
      Description: Provides a summary view of the lambda data
      Name: rds-usage-summary-view
      QueryString: !Sub |
        SELECT dbinstanceidentifier, 'memory' as Metric, memory.timestamp, memory.maximum
        FROM ${DatabaseName}.rds_usage_data
        cross join unnest(datapoints.FreeableMemory) as t(memory)
        union
        SELECT dbinstanceidentifier, 'cpu' as Metric, cpu.timestamp, cpu.maximum
        FROM ${DatabaseName}.rds_usage_data
        cross join unnest(datapoints.CPUUtilization) as t(cpu)
        union
        SELECT dbinstanceidentifier, 'NetworkReceiveThroughput' as Metric, NetworkReceiveThroughput.timestamp, NetworkReceiveThroughput.maximum
        FROM ${DatabaseName}.rds_usage_data
        cross join unnest(datapoints.NetworkReceiveThroughput) as t(NetworkReceiveThroughput)
        union
        SELECT dbinstanceidentifier, 'NetworkTransmitThroughput' as Metric, NetworkTransmitThroughput.timestamp, NetworkTransmitThroughput.maximum
        FROM ${DatabaseName}.rds_usage_data
        cross join unnest(datapoints.CPUUtilization) as t(NetworkTransmitThroughput)
        union
        SELECT dbinstanceidentifier, 'ReadIOPS' as Metric, ReadIOPS.timestamp, ReadIOPS.maximum
        FROM ${DatabaseName}.rds_usage_data
        cross join unnest(datapoints.ReadIOPS) as t(ReadIOPS)
        union
        SELECT dbinstanceidentifier, 'WriteIOPS' as Metric, WriteIOPS.timestamp, WriteIOPS.maximum
        FROM ${DatabaseName}.rds_usage_data
        cross join unnest(datapoints.WriteIOPS) as t(WriteIOPS)
        union
        SELECT dbinstanceidentifier, 'FreeStorageSpace' as Metric, FreeStorageSpace.timestamp, FreeStorageSpace.maximum
        FROM ${DatabaseName}.rds_usage_data
        cross join unnest(datapoints.FreeStorageSpace) as t(FreeStorageSpace)

  GravitonMappingTable:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref DatabaseName
      Description: Provides a summary view of the lambda data
      Name: graviton_mapping
      QueryString: !Sub |
        CREATE EXTERNAL TABLE `rds_graviton_mapping`(
          `dbtype` varchar(255),
          `databaseengine` varchar(255),
          `instancetype` varchar(255),
          `graviton_instancetype` varchar(255))
          ROW FORMAT DELIMITED
          FIELDS TERMINATED BY ','
        STORED AS INPUTFORMAT
          'org.apache.hadoop.mapred.TextInputFormat'
        OUTPUTFORMAT
          'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
        LOCATION
          's3://${DestinationBucket}/pricing/graviton'
        TBLPROPERTIES (
          'classification'='csv',
          'transient_lastDdlTime'='1666191659')

  GravitonAthenaQuery:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref DatabaseName
      Description: Provides a view of modernization opportunities for Graviton
      Name: rds-usage-rds-graviton
      QueryString: !Sub |
        WITH
        raw_data AS (
          SELECT dbinstanceidentifier,
            dbinstanceclass,
            concat(
              split_part(dbinstanceclass, '.', 1),
              '.',
              split_part(dbinstanceclass, '.', 2)
            ) instance_family,
            availabilityzone,
            rtrim(availabilityzone, 'abcdef') region,
            (CASE
                WHEN (multiaz = false) THEN 'Single-AZ'
                WHEN (multiaz = true) THEN 'Multi-AZ' ELSE 'Error check deploymentoption'
              END
            ) deploymentoption,
            (CASE
                WHEN (engine LIKE 'aurora%') THEN 'Aurora' ELSE 'AmazonRDS'
              END
            ) rds_type,
            engine,
            (CASE
                WHEN (engine LIKE '%postgres%') THEN 'PostgreSQL'
                WHEN (engine LIKE '%mysql%') THEN 'MySQL'
                WHEN (engine LIKE '%mariadb%') THEN 'MariaDB'
                WHEN (engine LIKE '%oracle%') THEN 'Oracle'
                WHEN (engine LIKE '%docdb%') THEN 'DocDB'
                WHEN (engine LIKE '%sqlserver%') THEN 'SQL Server'
                WHEN (engine LIKE 'aurora 5.6.10a') THEN 'aurora 5.6.10a'
                WHEN (engine LIKE '%neptune%') THEN 'Neptune' ELSE 'Check DB Engine'
              END) db_engine,
            (CASE
                WHEN (engineversion LIKE '%aurora%') THEN concat(
                  split_part(substr(engineversion, 18, 6), '.', 1),
                  '.',
                  split_part(substr(engineversion, 18, 6), '.', 2),
                  '.',
                  split_part(substr(engineversion, 18, 6), '.', 3)
                ) ELSE engineversion
              END) engineversion,
            (CASE
                WHEN (engineversion LIKE '%aurora%') THEN split_part(substr(engineversion, 18, 6), '.', 1) ELSE split_part(engineversion, '.', 1)
              END) major,
            (CASE
                WHEN (engineversion LIKE '%aurora%') THEN split_part(substr(engineversion, 18, 6), '.', 2) ELSE split_part(engineversion, '.', 2)
              END) minor,
            (CASE
                WHEN (engineversion LIKE '%aurora%') THEN split_part(substr(engineversion, 18, 6), '.', 3) ELSE split_part(engineversion, '.', 3)
              END) fix,
            accountid,
            year,
            month,
            CAST("concat"("year", '-', "month", '-01') AS date) "billing_period"
          FROM rds_usage_data),
        rds_pricing AS (
          SELECT (CASE
                WHEN ("database engine" LIKE 'Aurora%') THEN 'Aurora'
                WHEN ("database engine" = 'Any') THEN 'Any' ELSE 'AmazonRDS'
              END
            ) rds_type,
            (CASE
                WHEN ("database engine" LIKE 'Aurora%') THEN split_part("database engine", ' ', 2) ELSE "database engine"
              END
            ) db_engine,
            "deployment option",
            location,
            "instance type",
            (CASE
                WHEN (priceperunit is null) THEN 0E0 ELSE CAST(priceperunit AS decimal(18,2))
              END
            ) priceperunit,
            unit
          FROM ${DatabaseName}.pricing_rds_data
          WHERE ((("location type" = 'AWS Region')AND (purchaseoption = ''))AND ("product family" = 'Database Instance'))),
        graviton_mapping AS (
          SELECT raw_data.dbinstanceidentifier,
            raw_data.rds_type,
            raw_data.db_engine,
            raw_data.dbinstanceclass,
            (CASE
                WHEN (raw_data.dbinstanceclass LIKE '%g.%') THEN 'Already Graviton'
                WHEN (raw_data.dbinstanceclass LIKE '%gd.%') THEN 'Already Graviton'
                WHEN (raw_data.dbinstanceclass LIKE '%serverless%') THEN 'Ineligible' ELSE (
                  CASE
                    WHEN (raw_data.rds_type = 'AmazonRDS') THEN (
                      CASE
                        WHEN (raw_data.db_engine = 'MySQL') THEN (
                          CASE
                            WHEN (CAST(raw_data.major AS integer) > 8) THEN 'Eligible'
                            WHEN (
                              (CAST(raw_data.major AS integer) = 8)
                              AND (CAST(raw_data.minor AS integer) > 0)
                            ) THEN 'Eligible'
                            WHEN (
                              (
                                (CAST(raw_data.major AS integer) = 8)
                                AND (CAST(raw_data.minor AS integer) = 0)
                              )
                              AND (CAST(raw_data.fix AS integer) >= 17)
                            ) THEN 'Eligible' ELSE 'Requires Update'
                          END
                        )
                        WHEN (raw_data.db_engine = 'PostgreSQL') THEN (
                          CASE
                            WHEN (CAST(raw_data.major AS integer) > 12) THEN 'Eligible'
                            WHEN (
                              (CAST(raw_data.major AS integer) = 12)
                              AND (CAST(raw_data.minor AS integer) >= 3)
                            ) THEN 'Eligible' ELSE 'Requires Update'
                          END
                        )
                        WHEN (raw_data.db_engine = 'MariaDB') THEN (
                          CASE
                            WHEN (CAST(raw_data.major AS integer) > 10) THEN 'Eligible'
                            WHEN (
                              (CAST(raw_data.major AS integer) = 10)
                              AND (CAST(raw_data.minor AS integer) > 4)
                            ) THEN 'Eligible'
                            WHEN (
                              (
                                (CAST(raw_data.major AS integer) = 10)
                                AND (CAST(raw_data.minor AS integer) = 4)
                              )
                              AND (CAST(raw_data.fix AS integer) >= 13)
                            ) THEN 'Eligible' ELSE 'Requires Update'
                          END
                        ) ELSE 'Ineligible'
                      END
                    )
                    WHEN (raw_data.rds_type = 'Aurora') THEN (
                      CASE
                        WHEN (raw_data.db_engine = 'MySQL') THEN (
                          CASE
                            WHEN (CAST(raw_data.major AS integer) > 2) THEN 'Eligible'
                            WHEN (
                              (CAST(raw_data.major AS integer) = 2)
                              AND (CAST(raw_data.minor AS integer) > 9)
                            ) THEN 'Eligible'
                            WHEN (
                              (
                                (CAST(raw_data.major AS integer) = 2)
                                AND (CAST(raw_data.minor AS integer) = 9)
                              )
                              AND (CAST(raw_data.fix AS integer) >= 2)
                            ) THEN 'Eligible' ELSE 'Ineligible'
                          END
                        )
                        WHEN (raw_data.db_engine = 'PostgreSQL') THEN (
                          CASE
                            WHEN (CAST(raw_data.major AS integer) > 12) THEN 'Eligible'
                            WHEN (
                              (CAST(raw_data.major AS integer) = 12)
                              AND (CAST(raw_data.minor AS integer) >= 4)
                            ) THEN 'Eligible' ELSE 'Ineligible'
                          END
                        ) ELSE 'Ineligible'
                      END
                    )
                  END
                )
              END
            ) graviton_eligible,
            rds_graviton_mapping.graviton_instancetype,
            raw_data.region,
            pricing_regionnames_data.regionname,
            raw_data.deploymentoption,
            raw_data.engineversion,
            raw_data.accountid,
            raw_data.year,
            raw_data.month,
            raw_data.billing_period
          FROM (
              (
                raw_data
                LEFT JOIN ${DatabaseName}.pricing_regionnames_data ON (raw_data.region = pricing_regionnames_data.region)
              )
              LEFT JOIN ${DatabaseName}.rds_graviton_mapping ON (
                (
                  (raw_data.rds_type = rds_graviton_mapping.dbtype)
                  AND (
                    raw_data.db_engine = rds_graviton_mapping.databaseengine
                  )
                )
                AND (
                  raw_data.dbinstanceclass = rds_graviton_mapping.instancetype
                )
              )
            )
        )
        SELECT graviton_mapping.*,
          PL1.priceperunit existing_unit_price,
          (PL1.priceperunit * 720) existing_monthly_price,
          PL2.priceperunit graviton_unit_price,
          (PL2.priceperunit * 720) graviton_montlhy_price,
          round(
            (
              (PL1.priceperunit * 720) - (PL2.priceperunit * 720)
            ),
            2
          ) monthly_savings,
          round(
            (
              (
                (PL1.priceperunit * 720) - (PL2.priceperunit * 720)
              ) * 12
            ),
            2
          ) estimated_annual_savings,
          round((1 - (PL2.priceperunit / PL1.priceperunit)), 2) percentage_savings
        FROM (
            (
              graviton_mapping
              LEFT JOIN rds_pricing PL1 ON (
                (
                  (
                    (
                      (graviton_mapping.rds_type = PL1.rds_type)
                      AND (graviton_mapping.db_engine = PL1.db_engine)
                    )
                    AND (
                      graviton_mapping.deploymentoption = PL1."deployment option"
                    )
                  )
                  AND (graviton_mapping.regionname = PL1.location)
                )
                AND (
                  graviton_mapping.dbinstanceclass = PL1."instance type"
                )
              )
            )
            LEFT JOIN rds_pricing PL2 ON (
              (
                (
                  (
                    (
                      graviton_mapping.graviton_instancetype = PL2."instance type"
                    )
                    AND (graviton_mapping.regionname = PL2.location)
                  )
                  AND (graviton_mapping.rds_type = PL2.rds_type)
                )
                AND (graviton_mapping.db_engine = PL2.db_engine)
              )
              AND (
                graviton_mapping.deploymentoption = PL2."deployment option"
              )
            )
          )


