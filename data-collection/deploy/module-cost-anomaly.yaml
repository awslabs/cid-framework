AWSTemplateFormatVersion: "2010-09-09"
Description: Retrieves AWS Cost Explorer Cost Anomalies details accross AWS organization
Parameters:
  DatabaseName:
    Type: String
    Description: Name of the Athena database to be created to hold lambda information
    Default: optimization_data
  DestinationBucket:
    Type: String
    Description: Name of the S3 Bucket that exists or needs to be created to hold costanomaly information
    AllowedPattern: (?=^.{3,63}$)(?!^(\d+\.)+\d+$)(^(([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])\.)*([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])$)
  DestinationBucketARN:
    Type: String
    Description: ARN of the S3 Bucket that exists or needs to be created to hold costanomaly information
  ManagementRoleName:
    Type: String
    Description: The name of the IAM role that will be deployed in the management account which can retrieve AWS Organization data. KEEP THE SAME AS WHAT IS DEPLOYED INTO MANAGEMENT ACCOUNT
  CFDataName:
    Type: String
    Description: The name of what this cf is doing.
    Default: cost-anomaly
  GlueRoleARN:
    Type: String
    Description: Arn for the Glue Crawler role
  Schedule:
    Type: String
    Description: EventBridge Schedule to trigger the data collection
    Default: "rate(1 day)"
  ResourcePrefix:
    Type: String
    Description: This prefix will be placed in front of all roles created. Note you may wish to add a dash at the end to make more readable
  LambdaAnalyticsARN:
    Type: String
    Description: Arn of lambda for Analytics
  AccountCollectorLambdaARN:
    Type: String
    Description: Arn of the Account Collector Lambda
  CodeBucket:
    Type: String
    Description: Source code bucket
  StepFunctionTemplate:
    Type: String
    Description: S3 key to the JSON template for the StepFunction
  StepFunctionExecutionRoleARN:
    Type: String
    Description: Common role for Step Function execution
  SchedulerExecutionRoleARN:
    Type: String
    Description: Common role for module Scheduler execution
  LambdaManageGlueTableARN:
    Type: String
    Description: ARN of a Lambda for Managing GlueTable
  DataBucketsKmsKeysArns:
    Type: String
    Description: "ARNs of KMS Keys for data buckets and/or Glue Catalog. Comma separated list, no spaces. Keep empty if data Buckets and Glue Catalog are not Encrypted with KMS. You can also set it to '*' to grant decrypt permission for all the keys."
    Default: ""

Conditions:
  NeedDataBucketsKms: !Not [ !Equals [ !Ref DataBucketsKmsKeysArns, "" ] ]

Outputs:
  StepFunctionARN:
    Description: ARN for the module's Step Function
    Value: !GetAtt ModuleStepFunction.Arn

Resources:
  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "${ResourcePrefix}${CFDataName}-LambdaRole"
      AssumeRolePolicyDocument:
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - !Sub "lambda.${AWS::URLSuffix}"
        Version: 2012-10-17
      ManagedPolicyArns:
        - !Sub "arn:${AWS::Partition}:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
      Path: /
      Policies:
        - PolicyName: !Sub "${CFDataName}-ManagementAccount-LambdaRole"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action: "sts:AssumeRole"
                Resource: !Sub "arn:${AWS::Partition}:iam::*:role/${ManagementRoleName}" # Need to assume a Read role in all Management accounts
        - !If
          - NeedDataBucketsKms
          - PolicyName: "KMS"
            PolicyDocument:
              Version: "2012-10-17"
              Statement:
                - Effect: "Allow"
                  Action:
                    - "kms:GenerateDataKey"
                  Resource: !Split [ ',', !Ref DataBucketsKmsKeysArns ]
          - !Ref AWS::NoValue
        - PolicyName: "S3-Access"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action:
                  - "s3:PutObject"
                  - "s3:GetObject"
                  - "s3:PutObjectAcl"
                Resource:
                  - !Sub "${DestinationBucketARN}/*"
              - Effect: "Allow"
                Action:
                  - "s3:ListBucket"
                Resource:
                  - !Sub "${DestinationBucketARN}"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28 # Resource found with an explicit name, this disallows updates that require replacement of this resource
            reason: "Need explicit name to identify role actions"

  LambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub "${ResourcePrefix}${CFDataName}-Lambda"
      Description: !Sub "Lambda function to retrieve ${CFDataName}"
      Runtime: python3.12
      Architectures: [x86_64]
      Code:
        ZipFile: |
          import os
          import json
          import logging
          from datetime import date, timedelta, datetime

          import boto3

          BUCKET = os.environ['BUCKET_NAME']
          ROLE_NAME = os.environ['ROLE_NAME']
          MODULE_NAME = os.environ['PREFIX']
          TMP_FILE = '/tmp/tmp.json'
          REGION = "us-east-1"

          logger = logging.getLogger(__name__)
          logger.setLevel(getattr(logging, os.environ.get('LOG_LEVEL', 'INFO').upper(), logging.INFO))

          def lambda_handler(event, context): #pylint: disable=unused-argument
              logger.info(f"Incoming event: {json.dumps(event)}")
              key = "account"
              if key not in event:
                  logger.error(f"Lambda event parameter '{key}' not defined (fatal) in {MODULE_NAME} module. Please do not trigger this Lambda manually. "
                      f"Find the corresponding {MODULE_NAME} state machine in Step Functions and trigger from there."
                  )
                  raise RuntimeError(f"(MissingParameterError) Lambda event missing '{key}' parameter")

              account = json.loads(event[key])
              main(account, ROLE_NAME, MODULE_NAME, BUCKET)

              return {
                  'statusCode': 200
              }

          def main(account, role_name, module_name, bucket):
              start_date, end_date = calculate_dates(bucket, s3_path=f'{module_name}/cost-anomaly-data/')
              logger.info(f'Using start_date={start_date}, end_date={end_date}')

              data_uploaded = False
              account_id = account["account_id"]
              records = get_api_data(role_name, account_id, start_date, end_date)
              if len(records) > 0:
                  count = process_records(records, TMP_FILE)
                  if count > 0:
                      upload_to_s3(account_id, bucket, module_name, TMP_FILE)
                      data_uploaded = True
              if not data_uploaded:
                  logger.info("No file uploaded because no new records were found")

          def get_api_data(role_name, account_id, start_date, end_date):
              results = []
              client = get_client_with_role(role_name, account_id, region=REGION, service="ce")
              next_token = None
              while True: # operation get_anomalies cannot be paginated
                  params = {
                      "DateInterval": {
                        'StartDate': str(start_date),
                        'EndDate': str(end_date)
                      },
                      "MaxResults": 100,
                  }
                  if next_token:
                      params['NextPageToken'] = next_token
                  response = client.get_anomalies(**params)
                  results += response['Anomalies']
                  if 'NextPageToken' in response:
                      next_token = response['NextPageToken']
                  else:
                      break
              logger.info(f"API results total {len(results)}")
              return results


          def process_records(records, tmp_file):
              count = 0
              with open(tmp_file, "w", encoding='utf-8') as f:
                  for record in records:
                      data = parse_record(record)
                      f.write(to_json(data) + "\n")
                      count += 1
              logger.info(f"Processed a total of {count} new records for account")
              return count

          def parse_record(record):
              logger.debug(f"Processing record {record}")
              result = {
                  'AnomalyId': get_value_by_path(record, 'AnomalyId'),
                  'AnomalyStartDate': get_value_by_path(record, 'AnomalyStartDate'),
                  'AnomalyEndDate': get_value_by_path(record, 'AnomalyEndDate'),
                  'DimensionValue': get_value_by_path(record, 'DimensionValue'),
                  'MaxImpact': get_value_by_path(record, 'Impact/MaxImpact'),
                  'TotalActualSpend': get_value_by_path(record, 'Impact/TotalActualSpend'),
                  'TotalExpectedSpend': get_value_by_path(record, 'Impact/TotalExpectedSpend'),
                  'TotalImpact': get_value_by_path(record, 'Impact/TotalImpact'),
                  'TotalImpactpercentage': float(get_value_by_path(record, 'Impact/TotalImpactPercentage', 0.0)),
                  'MonitorArn': get_value_by_path(record, 'MonitorArn'),
                  'LinkedAccount': get_value_by_path(record, 'RootCauses/0/LinkedAccount'), # for backward compatibility
                  'LinkedAccountName': get_value_by_path(record, 'RootCauses/0/LinkedAccountName'), # for backward compatibility
                  'Region': get_value_by_path(record, 'RootCauses/0/Region'), # for backward compatibility
                  'Service': get_value_by_path(record, 'RootCauses/0/Service'), # for backward compatibility
                  'UsageType': get_value_by_path(record, 'RootCauses/0/UsageType'), # for backward compatibility
                  'RootCauses': get_value_by_path(record, 'RootCauses', []),  # Store full array of root causes
              }
              logger.debug("Processing record complete")
              return result


          def upload_to_s3(payer_id, bucket, module_name, tmp_file):
              key = datetime.now().strftime(f"{module_name}/{module_name}-data/payer_id={payer_id}/year=%Y/month=%m/day=%d/%Y-%m-%d.json")
              boto3.client('s3').upload_file(tmp_file, bucket, key)
              logger.info(f"Data stored to s3://{bucket}/{key}")


          def get_value_by_path(data, path, default=None):
              logger.debug(f"Traversing for path {path}")
              keys = path.split("/")
              current = data
              for key in keys:
                  if isinstance(current, dict) and key in current:
                      current = current.get(key, default)
                  elif isinstance(current, list) and key.isdigit():
                      try:
                          current = current[int(key)]
                      except IndexError:
                          logger.debug(f"Index value {key} within path {path} is not valid in get_value_by_path for data {data}, returning default of {default}")
                          return default
                  else:
                      logger.debug(f"Key value {key} within path {path} is not valid in get_value_by_path for data {data}, returning default of {default}")
                      return default
              return current


          def get_client_with_role(role_name, account_id, service, region):
              logger.debug(f"Attempting to get '{service}' client with role '{role_name}' from account '{account_id}' in region '{region}'")
              partition = boto3.session.Session().get_partition_for_region(region_name=region)
              credentials = boto3.client('sts').assume_role(
                  RoleArn=f"arn:{partition}:iam::{account_id}:role/{role_name}",
                  RoleSessionName="data_collection"
              )['Credentials']
              logger.debug("Successfully assumed role, now getting client")
              client = boto3.client(
                  service,
                  region_name = region,
                  aws_access_key_id=credentials['AccessKeyId'],
                  aws_secret_access_key=credentials['SecretAccessKey'],
                  aws_session_token=credentials['SessionToken'],
              )
              logger.debug(f"Successfully created '{service}' client with role '{role_name}' from account '{account_id}' in region '{region}'")
              return client

          def to_json(obj):
              return json.dumps(
                  obj,
                  default=lambda x:
                      x.isoformat() if isinstance(x, (date, datetime)) else None
              )

          def calculate_dates(bucket, s3_path):
              end_date = datetime.now().date()
              start_date = datetime.now().date() - timedelta(days=90) #Cost anomalies are available for last 90days
              # Check the create time of objects in the S3 bucket
              paginator = boto3.client('s3').get_paginator('list_objects_v2')
              contents = sum( [page.get('Contents', []) for page in paginator.paginate(Bucket=bucket, Prefix=s3_path)], [])
              last_modified_date = get_last_modified_date(contents)
              if last_modified_date and last_modified_date >= start_date:
                  start_date = last_modified_date
              return start_date, end_date

          def get_last_modified_date(contents):
              last_modified_dates = [obj['LastModified'].date() for obj in contents]
              last_modified_dates_within_90_days = [date for date in last_modified_dates if date >= datetime.now().date() - timedelta(days=90)]
              if last_modified_dates_within_90_days:
                  return max(last_modified_dates_within_90_days)
              return None
      Handler: "index.lambda_handler"
      MemorySize: 2688
      Timeout: 600
      Role: !GetAtt LambdaRole.Arn
      Environment:
        Variables:
          BUCKET_NAME: !Ref DestinationBucket
          PREFIX: !Ref CFDataName
          ROLE_NAME: !Ref ManagementRoleName
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W89 # Lambda functions should be deployed inside a VPC
            reason: "No need for VPC in this case"
          - id: W92 #  Lambda functions should define ReservedConcurrentExecutions to reserve simultaneous executions
            reason: "No need for simultaneous execution"

  LogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub "/aws/lambda/${LambdaFunction}"
      RetentionInDays: 60

  Crawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Sub "${ResourcePrefix}${CFDataName}-Crawler"
      Role: !Ref GlueRoleARN
      DatabaseName: !Ref DatabaseName
      Targets:
        S3Targets:
          - Path: !Sub "s3://${DestinationBucket}/${CFDataName}/${CFDataName}-data/"
      Configuration: |
        {
          "Version": 1.0,
          "Grouping": {
            "TableGroupingPolicy": "CombineCompatibleSchemas"
          },
          "CrawlerOutput": {
            "Tables": {
              "TableThreshold": 1
            }
          }
        }

  ModuleGlueTable:
    Type: Custom::ManageGlueTable
    Properties:
      ServiceToken: !Ref LambdaManageGlueTableARN
      TableInput:
        Name: cost_anomaly_data
        TableType: EXTERNAL_TABLE
        Parameters:
          classification: json
          compressionType: none
        PartitionKeys:
          - Name: payer_id
            Type: string
          - Name: year
            Type: string
          - Name: month
            Type: string
          - Name: day
            Type: string
        StorageDescriptor:
          Columns:
            - Name: anomalyid
              Type: string
            - Name: anomalystartdate
              Type: string
            - Name: anomalyenddate
              Type: string
            - Name: dimensionvalue
              Type: string
            - Name: maximpact
              Type: double
            - Name: totalactualspend
              Type: double
            - Name: totalexpectedspend
              Type: double
            - Name: totalimpact
              Type: double
            - Name: totalimpactpercentage
              Type: double
            - Name: monitorarn
              Type: string
            - Name: linkedaccount
              Type: string
            - Name: linkedaccountname
              Type: string
            - Name: region
              Type: string
            - Name: service
              Type: string
            - Name: usagetype
              Type: string
          InputFormat: org.apache.hadoop.mapred.TextInputFormat
          Location: !Sub "s3://${DestinationBucket}/${CFDataName}/${CFDataName}-data/"
          OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
          SerdeInfo:
            Parameters:
              paths: anomalyid,anomalystartdate,anomalyenddate,dimensionvalue,maximpact,totalactualspend,totalexpectedspend,totalimpact,totalimpactpercentage,monitorarn,linkedaccount,linkedaccountname,region,service,usagetype
            SerializationLibrary: org.openx.data.jsonserde.JsonSerDe

  ModuleStepFunction:
    Type: AWS::StepFunctions::StateMachine
    Properties:
      StateMachineName: !Sub "${ResourcePrefix}${CFDataName}-StateMachine"
      StateMachineType: STANDARD
      RoleArn: !Ref StepFunctionExecutionRoleARN
      DefinitionS3Location:
        Bucket: !Ref CodeBucket
        Key: !Ref StepFunctionTemplate
      DefinitionSubstitutions:
        AccountCollectorLambdaARN: !Ref AccountCollectorLambdaARN
        ModuleLambdaARN: !GetAtt LambdaFunction.Arn
        Crawlers: !Sub '["${ResourcePrefix}${CFDataName}-Crawler"]'
        CollectionType: "Payers"
        Params: ""
        Module: !Ref CFDataName
        DeployRegion: !Ref AWS::Region
        Account: !Ref AWS::AccountId
        Prefix: !Ref ResourcePrefix
        Bucket: !Ref DestinationBucket

  ModuleRefreshSchedule:
    Type: "AWS::Scheduler::Schedule"
    Properties:
      Description: !Sub "Scheduler for the ODC ${CFDataName} module"
      Name: !Sub "${ResourcePrefix}${CFDataName}-RefreshSchedule"
      ScheduleExpression: !Ref Schedule
      State: ENABLED
      FlexibleTimeWindow:
        MaximumWindowInMinutes: 30
        Mode: "FLEXIBLE"
      Target:
        Arn: !GetAtt ModuleStepFunction.Arn
        RoleArn: !Ref SchedulerExecutionRoleARN

  AnalyticsExecutor:
    Type: Custom::LambdaAnalyticsExecutor
    Properties:
      ServiceToken: !Ref LambdaAnalyticsARN
      Name: !Ref CFDataName
