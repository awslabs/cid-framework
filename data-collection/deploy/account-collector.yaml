AWSTemplateFormatVersion: '2010-09-09'
Description: Organization data collections.
Parameters:
  ManagementRoleName:
    Type: String
    Description: The name of the IAM role that will be deployed in the management account which can retrieve AWS Organization data. KEEP THE SAME AS WHAT IS DEPLOYED INTO MANAGEMENT ACCOUNT
  ManagementAccountID:
    Type: String
    AllowedPattern: ([a-z0-9\-, ]*?$)
    Description: "(Ex: 123456789,098654321,789054312) List of Payer IDs you wish to collect data for. Can just be one Accounts"
  ResourcePrefix:
    Type: String
    Description: This prefix will be placed in front of all roles created. Note you may wish to add a dash at the end to make more readable
  DestinationBucket:
    Type: String
    Description: Name of the S3 Bucket to be created to hold data information
    AllowedPattern: (?=^.{3,63}$)(?!^(\d+\.)+\d+$)(^(([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])\.)*([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])$)
  DestinationBucketARN:
    Type: String
    Description: ARN of the S3 Bucket that exists or needs to be created to hold rightsizing information
  DataBucketsKmsKeysArns:
    Type: String
    Description: "ARNs of KMS Keys for data buckets and/or Glue Catalog. Comma separated list, no spaces. Keep empty if data Buckets and Glue Catalog are not Encrypted with KMS. You can also set it to '*' to grant decrypt permission for all the keys."
    Default: ""
Outputs:
  LambdaFunctionName:
    Value: !Ref LambdaFunction
  LambdaFunctionARN:
    Description: Lambda function ARN
    Value: !GetAtt LambdaFunction.Arn
    Export:
      Name: !Sub ${ResourcePrefix}AccountCollectorLambdaARN
Conditions:
  NeedDataBucketsKms: !Not [ !Equals [ !Ref DataBucketsKmsKeysArns, "" ] ]
Resources:
  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "${ResourcePrefix}account-collector-LambdaRole"
      AssumeRolePolicyDocument:
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - !Sub "lambda.${AWS::URLSuffix}"
        Version: 2012-10-17
      ManagedPolicyArns:
        - !Sub "arn:${AWS::Partition}:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
      Path: /
      Policies:
        - PolicyName: "AssumeManagementRole"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action: "sts:AssumeRole"
                Resource: !Sub "arn:${AWS::Partition}:iam::*:role/${ManagementRoleName}" # Need to assume a Read role in all Management Accounts
        - PolicyName: "CloudWatch"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action:
                  - "logs:CreateLogGroup"
                  - "logs:CreateLogStream"
                  - "logs:PutLogEvents"
                  - "logs:DescribeLogStreams"
                Resource: !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:log-group:/*"
        - PolicyName: "SSM"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action:
                  - "ssm:GetParameter"
                Resource: !Sub "arn:${AWS::Partition}:ssm:${AWS::Region}:${AWS::AccountId}:parameter/cid/${ResourcePrefix}*"
        - PolicyName: "Lambda"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action:
                  - "lambda:GetAccountSettings"
                Resource: "*"
        - PolicyName: "S3-Access"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action:
                  - "s3:GetObject"
                  - "s3:PutObject"
                Resource:
                  - !Sub "${DestinationBucketARN}/*"
        - !If
          - NeedDataBucketsKms
          - PolicyName: "KMS"
            PolicyDocument:
              Version: "2012-10-17"
              Statement:
                - Effect: "Allow"
                  Action:
                    - "kms:GenerateDataKey"
                  Resource: !Split [ ',', !Ref DataBucketsKmsKeysArns ]
          - !Ref AWS::NoValue

    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28 # Resource found with an explicit name, this disallows updates that require replacement of this resource
            reason: "Need explicit name to identify role actions"

  LambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ResourcePrefix}account-collector-Lambda'
      Description: "Lambda function to retrieve the account list"
      Runtime: python3.12
      Architectures: [x86_64]
      Code:
          ZipFile: |
            import os
            import json
            import uuid
            import logging
            from datetime import datetime
            from functools import partial

            import boto3

            ROLE_NAME = os.environ.get('ROLE_NAME')
            RESOURCE_PREFIX = os.environ.get('RESOURCE_PREFIX')
            MANAGEMENT_ACCOUNT_IDS = os.environ.get('MANAGEMENT_ACCOUNT_IDS')
            BUCKET = os.environ.get('BUCKET_NAME')
            ROLE_NAME = os.environ.get('ROLE_NAME')
            RESOURCE_PREFIX = os.environ.get('RESOURCE_PREFIX')
            MANAGEMENT_ACCOUNT_IDS = os.environ.get('MANAGEMENT_ACCOUNT_IDS')
            BUCKET = os.environ.get('BUCKET_NAME')
            PREDEF_ACCOUNT_LIST_KEY = os.environ.get('PREDEF_ACCOUNT_LIST_KEY')
            LINKED_ACCOUNT_LIST_KEY = os.environ.get('LINKED_ACCOUNT_LIST_KEY')
            PAYER_ACCOUNT_LIST_KEY = os.environ.get('PAYER_ACCOUNT_LIST_KEY')
            EXCLUDED_ACCOUNT_LIST_KEY = os.environ.get('EXCLUDED_ACCOUNT_LIST_KEY')
            TMP_FILE = "/tmp/data.json"
            PREFIX = "account-collector"
            START_TIME = str(datetime.now().isoformat())

            logger = logging.getLogger(__name__)
            logger.setLevel(getattr(logging, os.environ.get('LOG_LEVEL', 'INFO').upper(), logging.INFO))

            def lambda_handler(event, context): #pylint: disable=unused-argument
                logger.info(f"Incoming event: {event}")
                sub_uuid = [context.aws_request_id, context.log_group_name, context.log_stream_name]
                account_type = event.get("Type", '').lower()
                module = event.get("Module", '').lower()
                run_uuid = event.get("RunUUID", str(uuid.uuid4()))
                params = event.get("Params", "")
                try:
                    # need to confirm that the Lambda concurrency limit is sufficient to avoid throttling
                    lambda_limit = boto3.client('lambda').get_account_settings()['AccountLimit']['ConcurrentExecutions']
                    if lambda_limit < 500:
                        raise Exception(STATUS_CONFLICT) #pylint: disable=broad-exception-raised

                    functions = { # keep keys same as boto3 services
                        'linked': iterate_linked_accounts,
                        'payers': partial(iterate_admins_accounts, None),
                        'organizations': partial(iterate_admins_accounts, 'organizations'),
                        'compute-optimizer': partial(iterate_admins_accounts, 'compute-optimizer'),
                        'backup': partial(iterate_admins_accounts, 'backup'),
                    }

                    if account_type not in functions:
                        account_type = None
                        raise Exception(STATUS_NOT_ACCEPTABLE) #pylint: disable=broad-exception-raised

                    account_iterator = functions[account_type]
                    logger.info(f"Looking for accounts")
                    with open(TMP_FILE, "w", encoding='utf-8') as f:
                        count = 0
                        f.write("[\n")
                        for account in account_iterator():
                            account['run_uuid'] = run_uuid
                            if count > 0:
                                f.write(",\n")
                            f.write(json.dumps(account))
                            count += 1
                        f.write("\n]")

                    if count == 0:
                        raise Exception(STATUS_NOT_FOUND) #pylint: disable=broad-exception-raised

                    key = f"{PREFIX}/{module+'-'+(params+'-' if params else '')+(LINKED_ACCOUNT_LIST_KEY if account_type == 'linked' else PAYER_ACCOUNT_LIST_KEY)}"
                    s3 = boto3.client('s3')
                    s3.upload_file(TMP_FILE, Bucket=BUCKET, Key=key)
                    location = f"s3://{BUCKET}/{key}"
                    log_entry = create_log_entry(module=module, module_function=PREFIX+'-lambda', params=params, region="us-east-1", record_count=count,
                                                location=location, run_uuid=run_uuid, record_context="account", sub_uuid=sub_uuid)
                    return {'statusCode': 200, 'accountList': key, 'bucket': BUCKET, 'logEntry': log_entry['logEntry']}
                except Exception as exc: #pylint: disable=broad-exception-caught
                    exc_msg = str(exc)
                    if exc_msg == str(STATUS_CONFLICT):
                        status_code = STATUS_CONFLICT
                        description = (f'Lambda concurrent executions limit of {lambda_limit} is not sufficient to run the Data Collection framework. '
                                    'Please increase the limit to at least 500 (1000 is recommended). '
                                    'See https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html.')
                    elif exc_msg == str(STATUS_NOT_ACCEPTABLE):
                        status_code = STATUS_NOT_ACCEPTABLE
                        description = f"Lambda event must have 'Type' parameter with value = ({list(functions.keys())})"
                    elif exc_msg == str(STATUS_NOT_FOUND):
                        status_code = STATUS_NOT_FOUND
                        description = 'No accounts found. Check the log.'
                    else:
                        status_code = None
                        description = None
                    create_log_entry(module=module, module_function=PREFIX+'-lambda', region="us-east-1", status_code=status_code, description=description, error=exc, run_uuid=run_uuid, sub_uuid=sub_uuid)
                    raise exc

            def get_all_payers():
                for payer_id in MANAGEMENT_ACCOUNT_IDS.split(','):
                    yield payer_id.strip()

            def iterate_admins_accounts(service=None):
                ssm = boto3.client('ssm')
                for payer_id in get_all_payers():
                    account_id = payer_id # default
                    if service:
                        ssm_key = f'/cid/{RESOURCE_PREFIX}config/delegated-admin/{service}/{payer_id}'
                        try:
                            account_id = ssm.get_parameter(Name=ssm_key)['Parameter']['Value']
                        except ssm.exceptions.ParameterNotFound:
                            logger.warning(f'Not found ssm parameter {ssm_key}. Will use Management Account Id {payer_id}')
                    yield {"account": json.dumps({'account_id': account_id, 'account_name': '', 'payer_id': payer_id}), "runuuid": ""}

            def iterate_linked_accounts():
                defined_accounts, ext = get_defined_list(BUCKET, PREDEF_ACCOUNT_LIST_KEY)
                if defined_accounts:
                    logger.info(f'Using defined account list found in s3://{BUCKET}/{PREDEF_ACCOUNT_LIST_KEY} instead of payer organization')
                    for account_data in defined_accounts:
                        if ext == ".json":
                            account = json.loads(account_data)
                            yield format_account(account['account_id'], account['account_name'], account['payer_id'])
                        else:
                            account = account_data.split(',')
                            yield format_account(account[0].strip(), account[1].strip(), account[2].strip())
                else:
                    logger.info('Using payer organization for the account list')
                    excluded_accounts = get_from_bucket(BUCKET, EXCLUDED_ACCOUNT_LIST_KEY)
                    if excluded_accounts:
                        logger.info(f'Found list of accounts to exclude in s3://{BUCKET}/{EXCLUDED_ACCOUNT_LIST_KEY}{ext}. Will only collect accounts that are not in the list')
                        excluded_accounts = [a.strip() for a in excluded_accounts[0].split(',') if a]
                    for org_account_data in iterate_admins_accounts('organizations'):
                        logger.info(f'Collecting accounts for payer {org_account_data}')
                        org_account = json.loads(org_account_data['account'])
                        payer_id = org_account['payer_id']
                        organizations = get_client_with_role(service="organizations", account_id=org_account['account_id'], region="us-east-1") #MUST be us-east-1
                        count = 0
                        for account in organizations.get_paginator("list_accounts").paginate().search("Accounts[?Status=='ACTIVE']"):
                            account_id = account.get('Id')
                            if excluded_accounts and account_id in excluded_accounts:
                                logger.debug(f'Excluding account {account_id}')
                                continue
                            count += 1
                            yield format_account(account_id, account.get('Name'), payer_id)
                        logger.info(f'Found {count} accounts for payer {payer_id}')

            def get_defined_list(bucket, key):
                s3 = boto3.client("s3")
                exts = [".json", ".csv"]
                for ext in exts:
                    accts = get_from_bucket(bucket, key+ext, s3)
                    accts = get_from_bucket(bucket, key+ext, s3)
                    if accts:
                        return accts, ext
                logger.debug(f'Predefined account list not retrieved or not being used')
                return None, None

            def get_from_bucket(bucket, key, client=None):
                s3 = client if client else boto3.client("s3")
                try:
                    data = s3.get_object(Bucket=bucket, Key=key)
                    return data['Body'].read().decode('utf-8').strip('\n').split('\n')
                except Exception: #pylint: disable=broad-exception-caught
                    return None

            def format_account(account_id, account_name, payer_id):
                return {
                    "account": json.dumps({
                        'account_id': account_id,
                        'account_name': account_name,
                        'payer_id': payer_id,
                    })
                }


            def get_client_with_role(account_id, service, region):
                partition = boto3.session.Session().get_partition_for_region(region_name=region)
                credentials = boto3.client('sts').assume_role(
                    RoleArn=f"arn:{partition}:iam::{account_id}:role/{ROLE_NAME}",
                    RoleSessionName="data_collection"
                )['Credentials']
                return boto3.client(
                    service,
                    region_name=region,
                    aws_access_key_id=credentials['AccessKeyId'],
                    aws_secret_access_key=credentials['SecretAccessKey'],
                    aws_session_token=credentials['SessionToken'],
                )

            def create_log_entry(payer_id="", account_id=None, start_time=None, status_code=None, region="", module=None, module_function="module-lambda", sub_code="",
                                params="", record_count=0, record_context="", description=None, location="", error=None, run_uuid="", sub_uuid=[], is_summary=False, store_it=True): # pylint: disable=too-many-locals
                """Format log entry for logging."""
                try:
                    # get the local account and region
                    dc_region = boto3.session.Session().region_name
                    dc_account_id = boto3.client('sts').get_caller_identity()['Account']
                except Exception as exc: #pylint: disable=broad-exception-caught
                    dc_region = ""
                    dc_account_id = ""
                    logger.error(f"{type(exc).__name__}: When trying to obtain local region and account information. Message: {str(exc)}")

                status_code, description = status_handler(error, record_count, is_summary, status_code, description, record_context)
                log_entry = {
                    "StartTime": start_time if start_time else START_TIME,
                    "EndTime":  str(datetime.now().isoformat()),
                    "DataCollectionRegion": dc_region,
                    "DataCollectionAccountId": dc_account_id,
                    "Module": module if module else PREFIX,
                    "ModuleFunction": module_function,
                    "Params": params,
                    "PayerId": payer_id,
                    "AccountId": account_id if account_id else payer_id,
                    "Region": region,
                    "StatusCode": status_code,
                    "SubCode": sub_code,
                    "RecordCount": record_count,
                    "Description": description,
                    "DataLocation": location if record_count > 0 else "",
                    "RunUUID": run_uuid,
                    "SubUUID": sub_uuid if isinstance(sub_uuid, list) else [sub_uuid],
                    "Service": "Lambda"
                }
                if status_code >= 400:
                    logger.error(description)
                if store_it:
                    store_log_entry(log_entry)
                logger.info(f"Result: {log_entry}")
                return {"statusCode": status_code, "logEntry": log_entry}

            def status_handler(error=None, record_count=0, is_summary=False, status_code=None, description="", record_context=""):
                """Codify status codes and descriptions for consistent logging."""
                if status_code:
                    return status_code, description
                if error:
                    exc_msg = str(error)
                    if exc_msg == str(STATUS_NOT_ACCEPTABLE):
                        return STATUS_NOT_ACCEPTABLE, ("InvocationError: Account parameters are not properly defined in request."
                            f"Please only trigger this Lambda from the corresponding StepFunction for this module.{' '+description if description else ''}")
                    if "AccessDenied" in exc_msg:
                        return STATUS_NOT_AUTHORIZED, f"AccessDenied: Unable to assume role {ROLE_NAME}. Please make sure the role exists. {exc_msg}"
                    if 'The security token included in the request is invalid' in exc_msg:
                        return STATUS_FORBIDDEN, f'{type(error).__name__}: Region might not be activated.'
                    return STATUS_SERVER_ERROR, f"{type(error).__name__}: with message {exc_msg}"
                # For all others, assume success
                description = f"Lambda execution successful: {record_count} {(record_context+' ') if record_context else ''}record{'s' if (record_count > 1 or record_count == 0) else ''} found.{' '+description if description else ''}"
                if is_summary:
                    return STATUS_MULTI_STATUS, "Multi-part "+description
                if record_count == 0:
                    return STATUS_OKAY_NO_CONTENT, description
                return STATUS_OKAY, description

            def store_log_entry(log_entry):
                """Store the log entry to S3."""
                key = datetime.now().strftime(f"logs/%Y/%m/%d/{PREFIX}-{uuid.uuid4()}.json")
                try:
                    boto3.client('s3').put_object(Body=json.dumps(log_entry), Bucket=BUCKET, Key=key)
                except Exception as exc: #pylint: disable=broad-exception-caught
                    logger.error(f"Error storing log entry to S3: {exc}")

            STATUS_OKAY = 200
            STATUS_OKAY_NO_CONTENT = 204
            STATUS_MULTI_STATUS = 207
            STATUS_NOT_AUTHORIZED = 401
            STATUS_FORBIDDEN = 403
            STATUS_NOT_FOUND = 404
            STATUS_NOT_ACCEPTABLE = 406
            STATUS_CONFLICT = 409
            STATUS_TOO_MANY_REQUESTS = 429
            STATUS_SERVER_ERROR = 500
            STATUS_NOT_IMPLEMENTED = 501
      Handler: 'index.lambda_handler'
      MemorySize: 2688
      Timeout: 600
      Role: !GetAtt LambdaRole.Arn
      Environment:
        Variables:
          ROLE_NAME: !Ref ManagementRoleName
          MANAGEMENT_ACCOUNT_IDS: !Ref ManagementAccountID
          RESOURCE_PREFIX: !Ref ResourcePrefix
          BUCKET_NAME: !Ref DestinationBucket
          PREDEF_ACCOUNT_LIST_KEY: "account-list/account-list"
          LINKED_ACCOUNT_LIST_KEY: "linked-account-list.json"
          PAYER_ACCOUNT_LIST_KEY: "payer-account-list.json"
          EXCLUDED_ACCOUNT_LIST_KEY: "excluded-linked-account-list.csv"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W89 # Lambda functions should be deployed inside a VPC
            reason: "No need for VPC in this case"
          - id: W92 #  Lambda functions should define ReservedConcurrentExecutions to reserve simultaneous executions
            reason: "No need for simultaneous execution"

  LogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub "/aws/lambda/${LambdaFunction}"
      RetentionInDays: 60
