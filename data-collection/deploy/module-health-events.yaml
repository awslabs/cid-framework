AWSTemplateFormatVersion: "2010-09-09"
Description: Retrieves AWS Health Events details across AWS organization
Parameters:
  DatabaseName:
    Type: String
    Description: Name of the Athena database to be created to hold lambda information
    Default: optimization_data
  DestinationBucket:
    Type: String
    Description: Name of the S3 Bucket that exists or needs to be created to hold Health Events information
    AllowedPattern: (?=^.{3,63}$)(?!^(\d+\.)+\d+$)(^(([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])\.)*([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])$)
  DestinationBucketARN:
    Type: String
    Description: ARN of the S3 Bucket that exists or needs to be created to hold Health Events information
  ManagementRoleName:
    Type: String
    Description: The name of the IAM role that will be deployed in the management account which can retrieve AWS Organization data. KEEP THE SAME AS WHAT IS DEPLOYED INTO MANAGEMENT ACCOUNT
  CFDataName:
    Type: String
    Description: The name of what this cf is doing.
    Default: health-events
  GlueRoleARN:
    Type: String
    Description: Arn for the Glue Crawler role
  Schedule:
    Type: String
    Description: EventBridge Schedule to trigger the data collection
    Default: "rate(1 day)"
  ResourcePrefix:
    Type: String
    Description: This prefix will be placed in front of all roles created. Note you may wish to add a dash at the end to make more readable
  LambdaAnalyticsARN:
    Type: String
    Description: Arn of lambda for Analytics
  AccountCollectorLambdaARN:
    Type: String
    Description: Arn of the Account Collector Lambda
  CodeBucket:
    Type: String
    Description: Source code bucket
  StepFunctionTemplate:
    Type: String
    Description: JSON representation of common StepFunction template
  StepFunctionExecutionRoleARN:
    Type: String
    Description: Common role for Step Function execution
  SchedulerExecutionRoleARN:
    Type: String
    Description: Common role for module Scheduler execution
  DetailStepFunctionTemplate:
    Type: String
    Description: JSON representation of the detail retrieval StepFunction template
  DataBucketsKmsKeysArns:
    Type: String
    Description: "ARNs of KMS Keys for data buckets and/or Glue Catalog. Comma separated list, no spaces. Keep empty if data Buckets and Glue Catalog are not Encrypted with KMS. You can also set it to '*' to grant decrypt permission for all the keys."
    Default: ""

Conditions:
  NeedDataBucketsKms: !Not [ !Equals [ !Ref DataBucketsKmsKeysArns, "" ] ]

Outputs:
  StepFunctionARN:
    Description: ARN for the module's Step Function
    Value: !GetAtt ModuleStepFunction.Arn

Resources:
  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "${ResourcePrefix}${CFDataName}-LambdaRole"
      AssumeRolePolicyDocument:
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - !Sub "lambda.${AWS::URLSuffix}"
        Version: 2012-10-17
      ManagedPolicyArns:
        - !Sub "arn:${AWS::Partition}:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
      Path: /
      Policies:
        - PolicyName: !Sub "${CFDataName}-ManagementAccount-LambdaRole"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action: "sts:AssumeRole"
                Resource: !Sub "arn:${AWS::Partition}:iam::*:role/${ManagementRoleName}" # Need to assume a Read role in all Management accounts
        - !If
          - NeedDataBucketsKms
          - PolicyName: "KMS"
            PolicyDocument:
              Version: "2012-10-17"
              Statement:
                - Effect: "Allow"
                  Action:
                    - "kms:GenerateDataKey"
                  Resource: !Split [ ',', !Ref DataBucketsKmsKeysArns ]
          - !Ref AWS::NoValue
        - PolicyName: "S3-Access"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action:
                  - "s3:PutObject"
                  - "s3:GetObject"
                  - "s3:PutObjectAcl"
                Resource:
                  - !Sub "${DestinationBucketARN}/*"
              - Effect: "Allow"
                Action:
                  - "s3:ListBucket"
                Resource:
                  - !Sub "${DestinationBucketARN}"
        - PolicyName: !Sub "${CFDataName}-detail-StateMachineExecution"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action: "states:StartExecution"
                Resource: !Sub "arn:${AWS::Partition}:states:${AWS::Region}:${AWS::AccountId}:stateMachine:${ResourcePrefix}${CFDataName}-detail-StateMachine"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28 # Resource found with an explicit name, this disallows updates that require replacement of this resource
            reason: "Need explicit name to identify role actions"

  LambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ResourcePrefix}${CFDataName}-Lambda'
      Description: !Sub "Lambda function to retrieve ${CFDataName}"
      Runtime: python3.12
      Architectures: [x86_64]
      Code:
        ZipFile: |
          #Module detail:
          # Name: Health Events
          # Account scope: Payers
          # Region(s): us-east-1 (default) or local active endpoint
          import os
          import json
          import uuid
          import logging
          import jmespath
          import socket
          from datetime import date, datetime, timedelta, timezone

          import boto3

          BUCKET = os.environ.get("BUCKET_NAME")
          PREFIX = os.environ.get("PREFIX")
          ROLE_NAME = os.environ.get('ROLE_NAME')
          TMP_FILE = "/tmp/data.json"
          START_TIME = str(datetime.now().isoformat())
          REGIONS = [r.strip() for r in os.environ.get("REGIONS", "").split(',') if r]
          if len(REGIONS) > 0:
              REGIONS.append('global')
          LOOKBACK = int(os.environ.get('LOOKBACK', "0"))
          DETAIL_SM_ARN = os.environ.get('DETAIL_SM_ARN')

          logger = logging.getLogger()
          logger.setLevel(getattr(logging, os.environ.get('LOG_LEVEL', 'INFO').upper(), logging.INFO))

          mapping = {
              'payer_account_id': 'payer_account_id',
              'account_id': 'awsAccountId',
              'event_code': 'event.eventTypeCode',
              'event_category': 'event.eventTypeCategory',
              'event_scope': 'event.eventScopeCode',
              'status_code': 'event.statusCode',
              'service': 'event.service',
              'region': 'event.region',
              'event_description': 'eventDescription.latestDescription',
              'affected_entity_value': 'entityValue',
              'affected_entity_arn': 'entityArn',
              'affected_entity_status_code': 'entityStatusCode',
              'affected_entity_last_update': 'entityLastUpdatedTime',
              'affected_entity_url': 'entityUrl',
              'availability_zone': 'event.availabilityZone',
              'deprecated_versions': 'deprecated_versions',
              'tags': 'tags',
              'start_time': 'event.startTime',
              'end_time': 'event.endTime',
              'last_updated_time': 'event.lastUpdatedTime',
              'event_metadata': 'eventMetadata',
              'event_source': 'event_source',
              'event_arn': 'event.arn',
              'ingestion_time': 'ingestion_time',
          }

          time_fields_to_convert = ['start_time', 'end_time', 'last_updated_time', 'affected_entity_last_update']

          def to_json(obj):
              """json helper for date, time and data"""
              def _date_transformer(obj):
                  return obj.isoformat() if isinstance(obj, (date, datetime)) else None
              return json.dumps(obj, default=_date_transformer)

          def chunks(lst, n):
              """Yield successive n-sized chunks from a list."""
              for i in range(0, len(lst), n):
                  yield lst[i:i + n]

          def event_item_to_date(event, keys):
              for key in keys:
                  if isinstance(event.get(key), int):
                      event[key] = int_to_datetime(event[key])
              return event

          def int_to_datetime(int_time):
              return datetime.datetime.utcfromtimestamp(int_time/1000)

          def iterate_paginated_results(client, function, search, params=None):
              yield from client.get_paginator(function).paginate(**(params or {})).search(search)

          def calculate_dates(bucket, s3_path):
              """ Timeboxes the range of events by seeking the most recent data collection date from the last 90 days """
              end_date = datetime.now(timezone.utc)
              start_date = end_date - timedelta(days=LOOKBACK)
              # Check the create time of objects in the S3 bucket
              contents = boto3.client('s3').get_paginator('list_objects_v2').paginate(
                  Bucket=bucket,
                  Prefix=s3_path
              ).search('Contents')
              start_date = max([obj['LastModified'] for obj in contents if obj] + [start_date])
              return start_date, end_date


          def search(function, args=None, expression='@'):
              compiled = jmespath.compile(expression)
              args = args or {}
              while True:
                  page = function(**args)
                  results = compiled.search(dict(page))
                  if isinstance(results, list):
                      yield from results
                  else:
                      # Yield result directly if it is not a list.
                      yield results
                  if 'nextToken' in page and page['nextToken']:
                      args['nextToken'] = page['nextToken']
                  else:
                      break

          def pull_event_details(event, health_client):
              event_arn = event['arn']
              if event['eventScopeCode'] == 'PUBLIC':
                  accounts = [None]
              else:
                  accounts = list(search(
                      function=health_client.describe_affected_accounts_for_organization,
                      args={'eventArn': event_arn},
                      expression='affectedAccounts',
                  ))

              # describe_event_details_for_organization only can get 10 per call
              details = []
              affected_entities = []
              for account_chunk in list(chunks(accounts, 10)):
                  if account_chunk[0]:
                      filters = [{'eventArn':event_arn, 'awsAccountId': account} for account in account_chunk]
                  else:
                      filters = [{'eventArn':event_arn}]
                  details += list(search(
                      function=health_client.describe_event_details_for_organization,
                      args=dict(
                          organizationEventDetailFilters=filters
                      ),
                      expression='successfulSet',
                  ))
                  affected_entities += list(search(
                      function=health_client.describe_affected_entities_for_organization,
                      args=dict(
                          organizationEntityFilters=filters
                      ),
                      expression='entities',
                  ))

              # merge with details and affected entities
              event_details_per_affected = []
              if len(affected_entities) == 0:
                  detail = details[0] if len(details) > 0 else {}
                  event = {**event, **detail}
                  event_details_per_affected.append(event)
              for affected_entity in affected_entities:
                  account = affected_entity['awsAccountId']
                  event_arn = affected_entity['eventArn']
                  affected_entity['entityStatusCode'] = affected_entity.pop('statusCode', None)
                  affected_entity['entityLastUpdatedTime'] = affected_entity.pop('lastUpdatedTime', None)
                  detail = jmespath.search(f"[?awsAccountId=='{account}']|[?event.arn=='{event_arn}']", details)
                  for detail_rec in detail:
                      metadata = detail_rec.get('eventMetadata') or {}
                      deprecated_versions = metadata.pop('deprecated_versions', None)
                      if deprecated_versions:
                          event['deprecated_versions'] = deprecated_versions
                      if len(metadata) == 0:
                          event['eventMetadata'] = ""
                  merged_dict = {**event, **affected_entity}
                  if len(detail) > 0:
                      merged_dict = {**merged_dict, **detail[0]}
                  event_details_per_affected.append(merged_dict)
              return event_details_per_affected

          def get_active_health_region():
              """
              Get the active AWS Health region from the global endpoint
              See: https://docs.aws.amazon.com/health/latest/ug/health-api.html#endpoints
              """

              default_region = "us-east-1"

              try:
                  (active_endpoint, _, _) = socket.gethostbyname_ex("global.health.amazonaws.com")
              except socket.gaierror:
                  return default_region

              split_active_endpoint = active_endpoint.split(".")
              if len(split_active_endpoint) < 2:
                  return default_region

              active_region = split_active_endpoint[1]
              return active_region

          def lambda_handler(event, context): #pylint: disable=unused-argument
              """ this lambda collects AWS Health Events data
              and must be called from the corresponding Step Function to orchestrate
              """
              logger.info(f"Event data: {event}")
              sub_uuid = [context.aws_request_id, context.log_group_name, context.log_stream_name]
              count = 0
              run_uuid = event.get("run_uuid", str(uuid.uuid4()))
              account_id = ""
              payer_id = ""

              try:
                  region = get_active_health_region()
                  account = event.get("account", "{}")
                  batch_input = event.get("BatchInput")
                  if batch_input:
                      is_summary_mode = False
                      account = batch_input.get("account", {})
                      run_type = 'detail'
                      items = event.get("Items")
                      run_uuid = batch_input.get("run_uuid", str(uuid.uuid4()))
                  else:
                      is_summary_mode = True
                      run_type = 'summary'
                      try:
                          account = json.loads(account)
                      except:
                          raise Exception(STATUS_NOT_ACCEPTABLE) #pylint: disable=broad-exception-raised
                  account_id = account.get("account_id")
                  logger.info(f"4 {account_id}")
                  payer_id = account_id
                  if not ((is_summary_mode and account_id) or (not is_summary_mode and batch_input and items)):
                      logger.info(f"5 {account_id}")
                      raise Exception(STATUS_NOT_ACCEPTABLE) #pylint: disable=broad-exception-raised

                  logger.info(f"Executing in {'summary' if is_summary_mode else 'detail'} mode flow")

                  partition = boto3.session.Session().get_partition_for_region(region_name=region)
                  creds = boto3.client('sts').assume_role(
                      RoleArn=f"arn:{partition}:iam::{account_id}:role/{ROLE_NAME}",
                      RoleSessionName="data_collection"
                  )['Credentials']
                  health_client = boto3.client(
                      'health',
                      region_name=region,
                      aws_access_key_id=creds['AccessKeyId'],
                      aws_secret_access_key=creds['SecretAccessKey'],
                      aws_session_token=creds['SessionToken'],
                  )

                  location = ""
                  if is_summary_mode:
                      start_from, start_to = calculate_dates(BUCKET, f"{PREFIX}/{PREFIX}-summary-data/payer_id={account_id}")
                      logger.info(f"Collecting events from {start_from} to {start_to}")
                      args = {
                          'maxResults':100,
                          'filter': {
                              'lastUpdatedTime': {
                                  'from': start_from.strftime('%Y-%m-%dT%H:%M:%S%z'),
                              },
                          }
                      }
                      if len(REGIONS) > 0:
                          args['filter']['regions'] = REGIONS

                      ingestion_time = datetime.now(timezone.utc)
                      with open(TMP_FILE, "w", encoding='utf-8') as f:
                          #f.write('eventArn,eventScopeCode\n')
                          event_list = []
                          for _, h_event in enumerate(search(health_client.describe_events_for_organization, args, expression='events')):
                              event_list.append(
                                      {
                                          'eventArn': h_event['arn'],
                                          'eventScopeCode': h_event['eventScopeCode']
                                      }
                                  )
                              count += 1
                          f.write(to_json(event_list))
                      if count > 0:
                          key = ingestion_time.strftime(f"{PREFIX}/{PREFIX}-summary-data/payer_id={account_id}/year=%Y/month=%m/day=%d/%Y-%m-%d.json")
                          location = f's3://{BUCKET}/{key}'
                          boto3.client('s3').upload_file(TMP_FILE, BUCKET, key)
                          # clear any previous runs for the same day
                          bucket = boto3.resource('s3').Bucket(BUCKET)
                          bucket.objects.filter(Prefix=ingestion_time.strftime(f"{PREFIX}/{PREFIX}-detail-data/payer_id={account_id}/year=%Y/month=%m/day=%d")).delete()
                          sf = boto3.client('stepfunctions')
                          sf_input = {
                              "bucket": BUCKET,
                              "file": key,
                              "account": account,
                              "ingestion_time": int(round(ingestion_time.timestamp())),
                              "run_uuid": run_uuid
                          }
                          sf_input = json.dumps(sf_input).replace('"', '\"') # need to escape the json for SF
                          sf.start_execution(stateMachineArn=DETAIL_SM_ARN, input=sf_input) # launch the detail gathering SF

                  elif items:
                      ingestion_time = datetime.fromtimestamp(int(batch_input.get('ingestion_time')))

                      with open(TMP_FILE, "w", encoding='utf-8') as f:
                          for item in items:
                              h_event = {'arn': item['eventArn'], 'eventScopeCode': item['eventScopeCode']}
                              h_event['payer_account_id'] = account_id
                              h_event['event_source'] = "aws.health"
                              h_event['ingestion_time'] = ingestion_time
                              all_detailed_events = pull_event_details(h_event, health_client)
                              flatten_events = jmespath.search("[].{"+', '.join([f'{k}: {v}' for k, v in mapping.items()]) + "}", all_detailed_events)
                              for flatten_event in flatten_events:
                                  flatten_event = event_item_to_date(flatten_event, time_fields_to_convert)
                                  # metadata structure can vary and cause schema change issues, force to string
                                  metadata = flatten_event.get('event_metadata')
                                  metadata = json.dumps(metadata) if (not isinstance(metadata, str)) and (metadata != None) else metadata
                                  flatten_event['event_metadata'] = metadata
                                  f.write(to_json(flatten_event) + '\n')
                                  count += 1
                      if count > 0:
                          rand = uuid.uuid4()
                          key = ingestion_time.strftime(f"{PREFIX}/{PREFIX}-detail-data/payer_id={account_id}/year=%Y/month=%m/day=%d/%Y-%m-%d-%H-%M-%S-{rand}.json")
                          location = f's3://{BUCKET}/{key}'
                          boto3.client('s3').upload_file(TMP_FILE, BUCKET, key)

                  return create_log_entry(payer_id=payer_id, account_id=account_id, region=region, record_count=count, params=run_type,
                                          location=location, run_uuid=run_uuid, sub_uuid=sub_uuid, record_context=f"{PREFIX} {run_type}")

              except Exception as exc: #pylint: disable=broad-exception-caught
                  is_not_activated = 'Organizational View feature is not enabled' in str(exc)
                  status_code = STATUS_NOT_IMPLEMENTED if is_not_activated else None
                  description = None if not is_not_activated else (f'Payer {account_id} does not have Organizational View. '
                                                                  'See https://docs.aws.amazon.com/health/latest/ug/enable-organizational-view-in-health-console.html')
                  return create_log_entry(payer_id=payer_id, account_id=account_id, region=region, params=run_type,
                                  status_code=status_code, description=description, record_count=count, error=exc, run_uuid=run_uuid, sub_uuid=sub_uuid)

          def create_log_entry(payer_id="", account_id=None, start_time=None, status_code=None, region="", module=None, module_function="module-lambda", sub_code="",
                              params="", record_count=0, record_context="", description=None, location="", error=None, run_uuid="", sub_uuid=[], is_summary=False, store_it=True): # pylint: disable=too-many-locals
              """Format log entry for logging."""
              try:
                  # get the local account and region
                  dc_region = boto3.session.Session().region_name
                  dc_account_id = boto3.client('sts').get_caller_identity()['Account']
              except Exception as exc: #pylint: disable=broad-exception-caught
                  dc_region = ""
                  dc_account_id = ""
                  logger.error(f"{type(exc).__name__}: When trying to obtain local region and account information. Message: {str(exc)}")

              status_code, description = status_handler(error, record_count, is_summary, status_code, description, record_context)
              log_entry = {
                  "StartTime": start_time if start_time else START_TIME,
                  "EndTime":  str(datetime.now().isoformat()),
                  "DataCollectionRegion": dc_region,
                  "DataCollectionAccountId": dc_account_id,
                  "Module": module if module else PREFIX,
                  "ModuleFunction": module_function,
                  "Params": params,
                  "PayerId": payer_id,
                  "AccountId": account_id if account_id else payer_id,
                  "Region": region,
                  "StatusCode": status_code,
                  "SubCode": sub_code,
                  "RecordCount": record_count,
                  "Description": description,
                  "DataLocation": location if record_count > 0 else "",
                  "RunUUID": run_uuid,
                  "SubUUID": sub_uuid if isinstance(sub_uuid, list) else [sub_uuid],
                  "Service": "Lambda"
              }
              if status_code >= 400:
                  logger.error(description)
              if store_it:
                  store_log_entry(log_entry)
              logger.info(f"Result: {log_entry}")
              return {"statusCode": status_code, "logEntry": log_entry}

          def status_handler(error=None, record_count=0, is_summary=False, status_code=None, description="", record_context=""):
              """Codify status codes and descriptions for consistent logging."""
              if status_code:
                  return status_code, description
              if error:
                  exc_msg = str(error)
                  if exc_msg == str(STATUS_NOT_ACCEPTABLE):
                      return STATUS_NOT_ACCEPTABLE, ("InvocationError: Account parameters are not properly defined in request."
                          f"Please only trigger this Lambda from the corresponding StepFunction for this module.{' '+description if description else ''}")
                  if "AccessDenied" in exc_msg:
                      return STATUS_NOT_AUTHORIZED, f"AccessDenied: Unable to assume role {ROLE_NAME}. Please make sure the role exists. {exc_msg}"
                  if 'The security token included in the request is invalid' in exc_msg:
                      return STATUS_FORBIDDEN, f'{type(error).__name__}: Region might not be activated.'
                  return STATUS_SERVER_ERROR, f"{type(error).__name__}: with message {exc_msg}"
              # For all others, assume success
              description = f"Execution successful: {record_count} {(record_context+' ') if record_context else ''}record{'s' if (record_count > 1 or record_count == 0) else ''} found.{' '+description if description else ''}"
              if is_summary:
                  return STATUS_MULTI_STATUS, "Multi-part "+description
              if record_count == 0:
                  return STATUS_OKAY_NO_CONTENT, description
              return STATUS_OKAY, description

          def store_log_entry(log_entry):
              """Store the log entry to S3."""
              key = datetime.now().strftime(f"logs/%Y/%m/%d/{PREFIX}-{uuid.uuid4()}.json")
              try:
                  boto3.client('s3').put_object(Body=json.dumps(log_entry), Bucket=BUCKET, Key=key)
              except Exception as exc: #pylint: disable=broad-exception-caught
                  logger.error(f"Error storing log entry to S3: {exc}")

          STATUS_OKAY = 200
          STATUS_OKAY_NO_CONTENT = 204
          STATUS_MULTI_STATUS = 207
          STATUS_NOT_AUTHORIZED = 401
          STATUS_FORBIDDEN = 403
          STATUS_NOT_FOUND = 404
          STATUS_NOT_ACCEPTABLE = 406
          STATUS_CONFLICT = 409
          STATUS_TOO_MANY_REQUESTS = 429
          STATUS_SERVER_ERROR = 500
          STATUS_NOT_IMPLEMENTED = 501
      Handler: "index.lambda_handler"
      MemorySize: 2688
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
      Environment:
        Variables:
          BUCKET_NAME: !Ref DestinationBucket
          PREFIX: !Ref CFDataName
          ROLE_NAME: !Ref ManagementRoleName
          #REGIONS: -- defining regions can miss events within the region list so default to global
          LOOKBACK: 730 # 2 years
          DETAIL_SM_ARN: !Sub 'arn:${AWS::Partition}:states:${AWS::Region}:${AWS::AccountId}:stateMachine:${ResourcePrefix}${CFDataName}-detail-StateMachine'
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W89 # Lambda functions should be deployed inside a VPC
            reason: "No need for VPC in this case"
          - id: W92 #  Lambda functions should define ReservedConcurrentExecutions to reserve simultaneous executions
            reason: "No need for simultaneous execution"

  LogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub "/aws/lambda/${LambdaFunction}"
      RetentionInDays: 60

  Crawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Sub '${ResourcePrefix}${CFDataName}-detail-Crawler'
      Role: !Ref GlueRoleARN
      DatabaseName: !Ref DatabaseName
      Targets:
        S3Targets:
          - Path: !Sub "s3://${DestinationBucket}/${CFDataName}/${CFDataName}-detail-data/"

  ModuleStepFunction:
    Type: AWS::StepFunctions::StateMachine
    Properties:
      StateMachineName: !Sub '${ResourcePrefix}${CFDataName}-StateMachine'
      StateMachineType: STANDARD
      RoleArn: !Ref StepFunctionExecutionRoleARN
      DefinitionS3Location:
        Bucket: !Ref CodeBucket
        Key: !Ref StepFunctionTemplate
      DefinitionSubstitutions:
        AccountCollectorLambdaARN: !Ref AccountCollectorLambdaARN
        ModuleLambdaARN: !GetAtt LambdaFunction.Arn
        Crawlers: '[]'
        CollectionType: 'Payers'
        Params: 'summary'
        Module: !Ref CFDataName
        DeployRegion: !Ref AWS::Region
        Account: !Ref AWS::AccountId
        Prefix: !Ref ResourcePrefix
        Bucket: !Ref DestinationBucket

  ModuleRefreshSchedule:
    Type: 'AWS::Scheduler::Schedule'
    Properties:
      Description: !Sub 'Scheduler for the ODC ${CFDataName} module'
      Name: !Sub '${ResourcePrefix}${CFDataName}-RefreshSchedule'
      ScheduleExpression: !Ref Schedule
      State: ENABLED
      FlexibleTimeWindow:
        MaximumWindowInMinutes: 30
        Mode: 'FLEXIBLE'
      Target:
        Arn: !GetAtt ModuleStepFunction.Arn
        RoleArn: !Ref SchedulerExecutionRoleARN

  StepFunctionDetail:
    Type: AWS::StepFunctions::StateMachine
    Properties:
      StateMachineName: !Sub '${ResourcePrefix}${CFDataName}-detail-StateMachine'
      StateMachineType: STANDARD
      RoleArn: !Ref StepFunctionExecutionRoleARN
      DefinitionS3Location:
        Bucket: !Ref CodeBucket
        Key: !Ref DetailStepFunctionTemplate
      DefinitionSubstitutions:
        ModuleLambdaARN: !GetAtt LambdaFunction.Arn
        Crawlers: !Sub '["${ResourcePrefix}${CFDataName}-detail-Crawler"]'
        Params: 'detail'
        Module: !Ref CFDataName
        DeployRegion: !Ref AWS::Region
        Account: !Ref AWS::AccountId
        Prefix: !Ref ResourcePrefix
        ItemsPerBatch: 50
        MaxConcurrentBatches: 1
        Partition: !Ref AWS::Partition
        Bucket: !Ref DestinationBucket
    Metadata:
      cfn-lint:
        config:
          ignore_checks:
            - E2532 #Passing a dynamic structure for crawlers input into the template as a DefinitionSubstitution
  AnalyticsExecutor:
    Type: Custom::LambdaAnalyticsExecutor
    Properties:
      ServiceToken: !Ref LambdaAnalyticsARN
      Name: !Ref CFDataName
